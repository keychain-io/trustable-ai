# Sprint Execution Monitoring Workflow

**Project**: {{ project.name }}
**Workflow**: Sprint Execution Monitoring
**Purpose**: Monitor sprint progress, identify blockers, and generate daily reports

## Output Formatting Requirements

**IMPORTANT**: Use actual Unicode emojis in reports, NOT GitHub-style shortcodes.

---

## Workflow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SPRINT EXECUTION - Implementation & Monitoring                             â”‚
â”‚                                                                             â”‚
â”‚  IMPLEMENTATION CYCLE (for each task):                                     â”‚
â”‚    1. /engineer â†’ Implement code + unit tests                              â”‚
â”‚    2. Run unit tests                                                       â”‚
â”‚    3. /tester â†’ Evaluate tests, run integration tests                      â”‚
â”‚    4. If tests pass with high confidence â†’ Auto-commit                     â”‚
â”‚    5. Update work item status                                              â”‚
â”‚                                                                             â”‚
â”‚  MONITORING CYCLE (daily):                                                 â”‚
â”‚    1. Collect sprint status data                                           â”‚
â”‚    2. /scrum-master â†’ Daily standup report                                 â”‚
â”‚    3. /senior-engineer â†’ Blocker analysis (if blocked items)               â”‚
â”‚    4. Quality health check                                                 â”‚
â”‚    5. /security-specialist â†’ Weekly security review                        â”‚
â”‚    6. Generate status report                                               â”‚
â”‚                                                                             â”‚
â”‚  Each agent command spawns a FRESH CONTEXT WINDOW via Task tool            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Initialize Workflow

```python
# Initialize work tracking adapter
import sys
sys.path.insert(0, ".claude/skills")
from work_tracking import get_adapter

adapter = get_adapter()
print(f"ðŸ“‹ Work Tracking: {adapter.platform}")

current_sprint = input("Sprint name (e.g., Sprint 1): ")

# Load sprint work items via adapter
try:
    sprint_items = adapter.query_work_items(
        filters={
            'System.IterationPath': f'{{ work_tracking.project }}\\{current_sprint}'
        }
    )
    print(f"ðŸ“‹ Found {len(sprint_items)} items in {current_sprint}")
except Exception as e:
    print(f"âŒ Failed to load sprint items: {e}")
    sprint_items = []
```

---

## PART A: IMPLEMENTATION CYCLE

### Implementation Cycle Overview

For each task in the sprint, follow this cycle:

1. **Engineer implements** â†’ Code + unit tests
2. **Run unit tests** â†’ Validate basic functionality
3. **Tester validates** â†’ Evaluate tests, run integration tests
4. **Auto-commit** â†’ If tests pass with high confidence
5. **Update work item** â†’ Mark as Done

---

### Step A1: Select Task to Implement

```python
# Get tasks that are ready to implement
ready_tasks = [
    item for item in sprint_items
    if item.get('state') in ['New', 'Approved', 'Ready']
    and item.get('type') == '{{ work_tracking.work_item_types.task }}'
]

if not ready_tasks:
    print("âœ… No tasks ready for implementation")
    # Skip to monitoring cycle
else:
    print(f"\nðŸ“‹ Tasks ready for implementation: {len(ready_tasks)}")
    for i, task in enumerate(ready_tasks[:10], 1):  # Show first 10
        title = task.get('title', 'Untitled')
        task_id = task.get('id')
        points = get_story_points(task)
        print(f"  {i}. #{task_id}: {title} ({points} pts)")

    # User selects task
    selection = input("\nSelect task number (or 'skip' for monitoring only): ")
    if selection.lower() == 'skip':
        selected_task = None
    else:
        try:
            idx = int(selection) - 1
            selected_task = ready_tasks[idx]
            print(f"\nâœ… Selected: #{selected_task['id']} - {selected_task['title']}")
        except (ValueError, IndexError):
            print("âŒ Invalid selection, skipping implementation")
            selected_task = None
```

---

### Step A2: Engineer Implementation

**IF A TASK IS SELECTED**, call `/engineer` with the following task:

```
## YOUR TASK: Implement Feature

Implement the task according to specifications.

### Task Details
- ID: {selected_task['id']}
- Title: {selected_task['title']}
- Description: {selected_task['description']}
- Acceptance Criteria: {selected_task['acceptance_criteria']}

### Project Context
- Language: {{ project.tech_stack.languages | join(', ') }}
- Frameworks: {{ project.tech_stack.frameworks | default([]) | join(', ') }}
- Source directory: {{ project.source_directory | default('src') }}
- Test directory: {{ project.test_directory | default('tests') }}

### Requirements
1. Implement ALL functionality per acceptance criteria
2. Follow existing code patterns in the project
3. Write unit tests for all new code
4. Ensure {{ quality_standards.test_coverage_min }}% coverage minimum

### Output
- Implementation files in appropriate locations
- Unit test file with comprehensive coverage
- All tests passing
```

**After the agent completes:**
- Verify implementation files created
- Note the files changed for commit later

---

### Step A3: Run Unit Tests

Run the test suite to validate basic functionality:

```bash
{% if 'Python' in project.tech_stack.languages %}
python -m pytest {{ project.test_directory | default('tests') }} -v
{% elif 'TypeScript' in project.tech_stack.languages %}
npm test
{% endif %}
```

**If unit tests fail:**
- Do NOT proceed to integration testing
- Review failures and fix implementation
- Re-run until unit tests pass

---

### Step A4: Tester Validation

**Call `/tester` with the following task:**

```
## YOUR TASK: Validate Implementation and Run Integration Tests

Evaluate the implementation, validate tests, and run integration tests.

### Task Being Validated
- ID: {selected_task['id']}
- Title: {selected_task['title']}
- Acceptance Criteria: {selected_task['acceptance_criteria']}

### Implementation Files
{List of files created/modified by engineer}

### Your Validation Steps

1. **Evaluate Unit Tests**
   - Do tests cover all acceptance criteria?
   - Are tests falsifiable (can they fail)?
   - Is coverage >= {{ quality_standards.test_coverage_min }}%?
   - Do tests check behavior, not implementation?

2. **Run Integration Tests**
   - Test feature in context of full system
   - Verify integration with existing components
   - Check for regressions
   - Validate error handling

3. **Quality Assessment**
   - Code complexity acceptable?
   - Security vulnerabilities?
   - Performance acceptable?

### Output Format

Return JSON with:
```json
{
  "validation_status": "pass|fail",
  "confidence": "high|medium|low",
  "test_results": {
    "unit_tests_pass": true|false,
    "integration_tests_pass": true|false,
    "coverage_percent": 85,
    "coverage_meets_standard": true|false
  },
  "issues_found": [
    "Description of any issues"
  ],
  "recommendation": "commit|fix_required"
}
```

**CRITICAL**: Set `confidence: "high"` ONLY if:
- All tests pass
- Coverage >= {{ quality_standards.test_coverage_min }}%
- No critical issues found
- Integration tests demonstrate feature works in full system
```

**After the agent completes:**
- Parse validation result JSON
- Check confidence level

---

### Step A5: Auto-Commit (High Confidence Only)

**IF `validation_status == "pass"` AND `confidence == "high"`**, auto-commit:

```bash
# Stage all implementation files
git add {{ project.source_directory | default('src') }}/ {{ project.test_directory | default('tests') }}/

# Create commit with task reference
git commit -m "$(cat <<'EOF'
Implement #{selected_task['id']}: {selected_task['title']}

{Brief summary of changes}

Acceptance criteria met:
{List acceptance criteria from task}

Test results:
- Unit tests: âœ… Pass
- Integration tests: âœ… Pass
- Coverage: {coverage_percent}% (>= {{ quality_standards.test_coverage_min }}%)

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>
EOF
)"

echo "âœ… Changes committed successfully"
```

**IF `confidence != "high"`**, do NOT commit:

```
âš ï¸  Tests passed but confidence is {confidence}.

Issues that need attention:
{List issues from validation}

Recommendation: {recommendation}

Changes NOT committed. Review issues and re-run validation.
```

---

### Step A6: Update Work Item Status

```python
# Update task status to Done (only if committed)
if committed:
    try:
        adapter.update_work_item(
            work_item_id=selected_task['id'],
            fields={
                'System.State': 'Done',
                'System.History': f"""
Implementation complete and committed.

Test Results:
- Unit Tests: Pass
- Integration Tests: Pass
- Coverage: {coverage_percent}%

Commit: {git_commit_hash}
"""
            }
        )
        print(f"âœ… Updated work item #{selected_task['id']} to Done")
    except Exception as e:
        print(f"âš ï¸ Failed to update work item: {e}")
        print(f"   Manual update required for #{selected_task['id']}")
```

---

## PART B: MONITORING CYCLE

### Step B1: Collect Sprint Status Data

Gather metrics from work items:

```python
# Calculate metrics from adapter work items
completed = [i for i in sprint_items if i.get('state') == 'Done']
in_progress = [i for i in sprint_items if i.get('state') == 'In Progress']
blocked = [i for i in sprint_items if i.get('state') == 'Blocked']
not_started = [i for i in sprint_items if i.get('state') == 'New']

# Get story points from work items
def get_story_points(item):
    {% if work_tracking.custom_fields.story_points %}
    return item.get('fields', {}).get('{{ work_tracking.custom_fields.story_points }}', 0) or 0
    {% else %}
    return 0
    {% endif %}

total_points = sum(get_story_points(i) for i in sprint_items)
completed_points = sum(get_story_points(i) for i in completed)

print(f"ðŸ“Š Sprint Status:")
print(f"  Total: {len(sprint_items)} items ({total_points} pts)")
print(f"  âœ… Done: {len(completed)} ({completed_points} pts)")
print(f"  ðŸ”„ In Progress: {len(in_progress)}")
print(f"  ðŸ”´ Blocked: {len(blocked)}")
print(f"  â¬œ Not Started: {len(not_started)}")
```

---

### Step B2: Generate Daily Standup Report

**Call `/scrum-master` with the following task:**

```
## YOUR TASK: Generate Daily Standup Report

Create a daily standup report for the team.

### Sprint Data
- Sprint: {current_sprint}
- Total Items: {len(sprint_items)}
- Completed: {len(completed)} ({completed_points} pts)
- In Progress: {len(in_progress)}
- Blocked: {len(blocked)}
- Not Started: {len(not_started)}

### Work Items
{List of all work items with status}

### Generate Report Including:

1. **Yesterday's Progress**
   - What was completed
   - Story points delivered

2. **Today's Focus**
   - What should be worked on
   - Priority items

3. **Blockers & Impediments**
   - Current blockers
   - Who needs to resolve them

4. **Sprint Health**
   - On track / At risk / Behind
   - Days remaining
   - Burndown status

5. **Recommendations**
   - Team focus areas
   - Risk mitigations

### Output Format

Return a formatted standup report in markdown.
```

**After the agent completes:**
- Display standup report to user
- Save to `.claude/reports/daily/`

---

### Step B3: Analyze Blockers (If Any)

**IF THERE ARE BLOCKED ITEMS**, call `/senior-engineer` with the following task:

```
## YOUR TASK: Analyze Blocked Work Items

Review blocked items and suggest resolutions.

### Blocked Items
{List of blocked work items with details}

### For Each Blocker, Analyze:

1. **Root Cause**
   - Why is this blocked?
   - Technical vs. organizational blocker

2. **Impact Assessment**
   - How many items depend on this?
   - Sprint goal impact

3. **Resolution Options**
   - Technical solutions or workarounds
   - Who needs to be involved
   - Estimated time to resolve

4. **Priority Ranking**
   - Which blockers to resolve first
   - Critical path analysis

### Output Format

Return JSON with blocker analysis and recommendations.
```

**After the agent completes:**
- Display blocker analysis
- Recommend actions to unblock items

---

### Step B4: Quality Health Check

Run automated quality checks:

```bash
{% if 'Python' in project.tech_stack.languages %}
# Run tests with coverage
python -m pytest --cov={{ project.source_directory | default('src') }} --cov-report=term

# Check coverage against standard
# Target: {{ quality_standards.test_coverage_min }}%
{% endif %}
```

Compare results against quality standards:
- Test Coverage: Current vs. {{ quality_standards.test_coverage_min }}%
- Critical Vulnerabilities: Current vs. {{ quality_standards.critical_vulnerabilities_max }}
- Code Complexity: Current vs. {{ quality_standards.code_complexity_max }}

---

### Step B5: Weekly Security Review (Fridays Only)

**FOR WEEKLY REPORTS ONLY**, call `/security-specialist` with the following task:

```
## YOUR TASK: Weekly Security Status Review

Review sprint security status.

### Quality Standards
- Critical Vulnerabilities: Max {{ quality_standards.critical_vulnerabilities_max }}
- High Vulnerabilities: Max {{ quality_standards.high_vulnerabilities_max }}

### Security Scan Results
{Include any security scan output}

### Analyze:

1. **Vulnerability Status**
   - New vulnerabilities this sprint
   - Resolved vulnerabilities
   - Outstanding issues

2. **Security Impact of Changes**
   - Features with security implications
   - Authentication/authorization changes

3. **Recommendations**
   - Critical issues requiring immediate attention
   - Security tasks for next sprint

### Output Format

Return security review report in markdown.
```

---

### Step B6: Generate Status Report

Compile comprehensive report:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“Š SPRINT STATUS REPORT - {current_sprint}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“ˆ Progress: {completed_points}/{total_points} points ({percentage}%)

ðŸ“‹ Work Items:
  âœ… Done: {done_count}
  ðŸ”„ In Progress: {in_progress_count}
  ðŸ”´ Blocked: {blocked_count}
  â¬œ Not Started: {not_started_count}

âš ï¸ Blockers:
  {blocker_list}

ðŸ”’ Quality:
  - Test Coverage: {coverage}% (target: {{ quality_standards.test_coverage_min }}%)
  - Vulnerabilities: {vuln_count}

ðŸŽ¯ Sprint Health: {On Track / At Risk / Behind}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Agent Commands Used

| Cycle | Step | Agent Command | Purpose |
|-------|------|---------------|---------|
| **Implementation** | A2 | `/engineer` | Implement code + unit tests |
| **Implementation** | A4 | `/tester` | Validate tests, run integration tests |
| **Monitoring** | B2 | `/scrum-master` | Daily standup report |
| **Monitoring** | B3 | `/senior-engineer` | Blocker analysis (when blocked) |
| **Monitoring** | B5 | `/security-specialist` | Weekly security review |

**Key**: Each agent command spawns a **fresh context window** via the Task tool.

**Auto-Commit**: Executes at step A5 when `validation_status == "pass"` AND `confidence == "high"`

---

## Execution Schedule

- **Daily (9 AM)**: Steps 1-4 (status, standup, blockers, quality)
- **Weekly (Friday 4 PM)**: Full workflow including security review
- **Ad-hoc**: Run manually when needed

---

## Configuration

**Work Tracking Platform:** {{ work_tracking.platform }}

**Quality Standards:**
- Test Coverage: >= {{ quality_standards.test_coverage_min }}%
- Critical Vulnerabilities: <= {{ quality_standards.critical_vulnerabilities_max }}
- Code Complexity: <= {{ quality_standards.code_complexity_max }}

---

*Generated by Trustable AI Workbench for {{ project.name }}*
