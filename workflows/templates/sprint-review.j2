# Sprint Review Workflow

**Project**: {{ project.name }}
**Workflow**: Sprint Review (Acceptance & Deployment Readiness)
**Purpose**: Review sprint completion, run acceptance tests, assess deployment readiness, and close sprint

## Output Formatting Requirements

**IMPORTANT**: Use actual Unicode emojis in reports, NOT GitHub-style shortcodes.

---

## Workflow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SPRINT REVIEW - Acceptance & Deployment Readiness                          â”‚
â”‚                                                                             â”‚
â”‚  Step 1: Collect sprint completion metrics                                 â”‚
â”‚  Step 1.5: Identify EPICs for testing                                      â”‚
â”‚  Step 2: /tester â†’ Run acceptance tests                                    â”‚
â”‚  Step 3: /security-specialist â†’ Final security review                      â”‚
â”‚  Step 4: /engineer â†’ Deployment readiness assessment                       â”‚
â”‚  Step 5: /scrum-master â†’ Sprint closure decision                           â”‚
â”‚  Step 6: Human approval â†’ Close sprint or extend                           â”‚
â”‚                                                                             â”‚
â”‚  Each agent command spawns a FRESH CONTEXT WINDOW via Task tool            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Initialize Workflow

```python
# Initialize work tracking adapter
import sys
from datetime import datetime
sys.path.insert(0, ".claude/skills")
from work_tracking import get_adapter

adapter = get_adapter()
print(f"ðŸ“‹ Work Tracking: {adapter.platform}")

sprint_name = input("Sprint name (e.g., Sprint 3): ")

# Load sprint work items
try:
    sprint_items = adapter.query_work_items(
        filters={
            'System.IterationPath': f'{{ work_tracking.project }}\\{sprint_name}'
        }
    )
    print(f"ðŸ“‹ Found {len(sprint_items)} items in {sprint_name}")
except Exception as e:
    print(f"âŒ Failed to load sprint items: {e}")
    sprint_items = []
```

---

## Step 1: Collect Sprint Completion Metrics

```python
# Calculate completion metrics
completed = [i for i in sprint_items if i.get('state') == 'Done']
in_progress = [i for i in sprint_items if i.get('state') == 'In Progress']
not_done = [i for i in sprint_items if i.get('state') not in ['Done', 'Removed']]

# Get story points
def get_story_points(item):
    {% if work_tracking.custom_fields.story_points %}
    return item.get('fields', {}).get('{{ work_tracking.custom_fields.story_points }}', 0) or 0
    {% else %}
    return 0
    {% endif %}

total_points = sum(get_story_points(i) for i in sprint_items)
completed_points = sum(get_story_points(i) for i in completed)
completion_rate = (completed_points / total_points * 100) if total_points > 0 else 0

print(f"\nðŸ“Š Sprint Completion Metrics:")
print(f"  Total Items: {len(sprint_items)} ({total_points} pts)")
print(f"  âœ… Completed: {len(completed)} ({completed_points} pts)")
print(f"  ðŸ”„ In Progress: {len(in_progress)}")
print(f"  â¬œ Not Done: {len(not_done)}")
print(f"  ðŸ“ˆ Completion Rate: {completion_rate:.1f}%")
```

---

## Step 1.5: Identify EPICs for Testing

**CRITICAL**: Query work tracking adapter for Epic work items scheduled for review in current sprint. Extract EPICs and verify each has attached/linked acceptance test plan. Only EPICs with test plans proceed to acceptance testing (VISION.md Pillar #2: External Source of Truth).

```python
import os
from pathlib import Path

# Identify EPICs in sprint scope
print(f"\nðŸ” Identifying EPICs for acceptance testing...")
print("=" * 80)
print("CRITICAL: Querying work tracking adapter for Epic work items in sprint scope")
print("=" * 80)

epic_ids = []
epic_data = []

# Query adapter for Epic work items in sprint
try:
    epic_items = adapter.query_work_items(
        filters={
            'System.WorkItemType': '{{ work_tracking.work_item_types.epic }}',
            'System.IterationPath': f'{{ work_tracking.project }}\\{sprint_name}'
        }
    )
    print(f"ðŸ“‹ Found {len(epic_items)} EPIC(s) in {sprint_name}")
except Exception as e:
    print(f"âš ï¸  Failed to query Epic work items: {e}")
    epic_items = []

# Extract EPIC IDs and metadata
for item in epic_items:
    try:
        epic_id = item.get('id')
        fields = item.get('fields', {})

        epic_metadata = {
            'id': epic_id,
            'title': item.get('title') or fields.get('System.Title', ''),
            'state': item.get('state') or fields.get('System.State', ''),
            'description': item.get('description') or fields.get('System.Description', '')
        }

        epic_ids.append(epic_id)
        epic_data.append(epic_metadata)
        print(f"âœ… Found EPIC #{epic_id}: {epic_metadata['title']}")

    except Exception as e:
        print(f"âš ï¸  Failed to extract EPIC metadata: {e}")

# Verify each EPIC has attached test plan
print(f"\nðŸ“‹ Verifying test plans for {len(epic_data)} EPIC(s)...")
print("=" * 80)

testable_epics = []
epics_without_test_plans = []

for epic in epic_data:
    epic_id = epic['id']
    epic_title = epic['title']
    test_plan_found = False

    print(f"\nðŸ” Checking EPIC #{epic_id}: {epic_title}")

    try:
        # Platform-specific test plan verification
        if adapter.platform == 'azure-devops':
            # Azure DevOps: Check work item attachments for test plan file
            print(f"   ðŸ”— Checking Azure DevOps attachments...")

            # Import Azure CLI wrapper (canonical source)
            import sys
            sys.path.insert(0, ".claude/skills")
            from azure_devops.cli_wrapper import azure_cli

            # Query work item for attachments
            work_item = adapter.get_work_item(epic_id)

            if work_item:
                # Check relations for attachments
                relations = work_item.get('relations', [])

                for relation in relations:
                    rel_type = relation.get('rel', '')

                    # Azure DevOps: AttachedFile relation type
                    if 'AttachedFile' in rel_type:
                        # Get attachment URL
                        attachment_url = relation.get('url', '')

                        # Check if attachment is a test plan (contains 'test-plan' in name)
                        if 'test-plan' in attachment_url.lower():
                            test_plan_found = True
                            print(f"   âœ… Test plan attachment found: {attachment_url}")
                            break

                if not test_plan_found:
                    # Also check for test plan file by expected name pattern
                    expected_filename = f"epic-{epic_id}-test-plan.md"
                    test_plan_found = azure_cli.verify_attachment_exists(
                        work_item_id=int(epic_id) if isinstance(epic_id, int) else int(str(epic_id).split('-')[-1]),
                        filename=expected_filename
                    )

                    if test_plan_found:
                        print(f"   âœ… Test plan file verified: {expected_filename}")

        elif adapter.platform == 'file-based':
            # File-based: Check work item comments for test plan path
            print(f"   ðŸ“ Checking file-based work item comments...")

            work_item = adapter.get_work_item(epic_id)

            if work_item:
                comments = work_item.get('comments', [])

                # Check if any comment contains test plan path
                for comment in comments:
                    comment_text = comment.get('text', '')

                    # Look for test plan path in comment
                    if 'Test Plan:' in comment_text or 'test-plan' in comment_text.lower():
                        test_plan_found = True
                        print(f"   âœ… Test plan reference found in comments")
                        break

                if not test_plan_found:
                    # Also check if test plan file exists at expected location
                    test_plan_path = Path(f".claude/acceptance-tests/epic-{epic_id}-test-plan.md")

                    if test_plan_path.exists():
                        test_plan_found = True
                        print(f"   âœ… Test plan file exists: {test_plan_path}")

        else:
            # Unsupported platform - check local file system
            print(f"   ðŸ“ Checking local file system...")
            test_plan_path = Path(f".claude/acceptance-tests/epic-{epic_id}-test-plan.md")

            if test_plan_path.exists():
                test_plan_found = True
                print(f"   âœ… Test plan file exists: {test_plan_path}")

        # Add EPIC to appropriate list
        if test_plan_found:
            testable_epics.append(epic)
            print(f"   âœ… EPIC #{epic_id} eligible for acceptance testing")
        else:
            epics_without_test_plans.append(epic)
            print(f"   âš ï¸  WARNING: EPIC #{epic_id} has no test plan - excluded from acceptance testing")

    except Exception as e:
        print(f"   âŒ ERROR: Failed to verify test plan for EPIC #{epic_id}: {e}")
        epics_without_test_plans.append(epic)

# Output EPIC identification summary
print(f"\nðŸ“Š EPIC Identification Summary:")
print(f"   Total EPICs found: {len(epic_data)}")
print(f"   EPICs with test plans: {len(testable_epics)}")
print(f"   EPICs without test plans: {len(epics_without_test_plans)}")

# Log warnings for EPICs without test plans
if epics_without_test_plans:
    print(f"\nâš ï¸  WARNING: {len(epics_without_test_plans)} EPIC(s) excluded from acceptance testing due to missing test plans:")
    for epic in epics_without_test_plans:
        print(f"     â€¢ EPIC #{epic['id']}: {epic['title']}")
    print(f"\n   These EPICs will be skipped during acceptance testing.")
    print(f"   To include them, attach/link test plans to the EPIC work items.")

# Store testable EPICs in workflow state
testable_epics_state = {
    'testable_epics': testable_epics,
    'epics_without_test_plans': epics_without_test_plans,
    'total_epics_found': len(epic_data),
    'testable_count': len(testable_epics),
    'identification_timestamp': datetime.now().isoformat()
}

# Checkpoint: Save testable EPICs to workflow state
# (In production, this would be saved via StateManager)
print(f"\nðŸ’¾ Testable EPICs stored in workflow state for acceptance testing")
print(f"=" * 80)
```

---

## Step 2: Run Acceptance Tests

**Call `/tester` with the following task:**

```
## YOUR TASK: Run Acceptance Tests for Sprint Completion

Execute acceptance tests for all completed features in the sprint.

### Sprint Details
- Sprint: {sprint_name}
- Completed Items: {len(completed)}
- Completed Story Points: {completed_points}

### Completed Work Items
{For each completed item:
  - ID: {item['id']}
  - Title: {item['title']}
  - Acceptance Criteria: {item['acceptance_criteria']}
}

### Acceptance Testing Requirements

1. **Functional Acceptance Tests**
   - Verify each completed feature meets its acceptance criteria
   - Test user workflows end-to-end
   - Validate integration with existing system
   - Check for regressions in existing functionality

2. **Non-Functional Acceptance Tests**
   - Performance: Response times within SLA
   - Scalability: Handles expected load
   - Usability: UI/UX meets standards
   - Compatibility: Works across supported platforms/browsers

3. **Data Validation**
   - Data migrations completed successfully
   - Data integrity maintained
   - No data loss or corruption

4. **Quality Gates**
   - Test coverage >= {{ quality_standards.test_coverage_min }}%
   - No critical or high priority bugs open
   - All tests passing
   - No security vulnerabilities

### Output Format

Return JSON with:
```json
{
  "acceptance_status": "pass|fail|partial",
  "tests_run": 50,
  "tests_passed": 48,
  "tests_failed": 2,
  "coverage_percent": 85,
  "failed_criteria": [
    {
      "work_item": "1234",
      "criterion": "User can reset password",
      "failure_reason": "Email not sent in dev environment"
    }
  ],
  "quality_gates": {
    "coverage_met": true,
    "no_critical_bugs": true,
    "all_tests_passing": false
  },
  "recommendation": "approve|fix_required|partial_approval"
}
```
```

**After the agent completes:**
- Parse acceptance test results
- Document any failures

---

## Step 3: Security Review

**Call `/security-specialist` with the following task:**

```
## YOUR TASK: Final Security Review Before Deployment

Perform final security review of all changes in the sprint.

### Sprint Changes
- Completed Features: {list of completed features}
- Code Changes: {git diff statistics}
- New Dependencies: {any new packages/libraries added}

### Security Review Checklist

1. **Vulnerability Scan Results**
   - Run dependency scanner (e.g., pip-audit, npm audit)
   - Check for known vulnerabilities in dependencies
   - Verify {{ quality_standards.critical_vulnerabilities_max }} critical vulns max
   - Verify {{ quality_standards.high_vulnerabilities_max }} high vulns max

2. **Code Security Review**
   - OWASP Top 10 compliance
   - Authentication/authorization changes reviewed
   - Input validation and sanitization
   - SQL injection prevention
   - XSS prevention
   - CSRF protection

3. **Configuration Security**
   - No secrets in code or config files
   - Proper environment variable usage
   - Secure default configurations
   - HTTPS enforced where required

4. **Deployment Security**
   - Container image vulnerabilities scanned
   - Infrastructure as code reviewed
   - Network security rules validated
   - Access controls configured correctly

### Output Format

Return security review report:
```markdown
## Security Review Report - {sprint_name}

### Summary
- âœ… Critical Vulnerabilities: {count} (Max: {{ quality_standards.critical_vulnerabilities_max }})
- âœ… High Vulnerabilities: {count} (Max: {{ quality_standards.high_vulnerabilities_max }})
- Overall Status: APPROVED | CONDITIONAL | REJECTED

### Vulnerabilities Found
{List with severity, component, fix required}

### Security Requirements for Deployment
{List any security controls that must be in place}

### Recommendations
{Security improvements for future sprints}
```
```

**After the agent completes:**
- Review security findings
- Address critical/high vulnerabilities if any

---

## Step 4: Deployment Readiness Assessment

**Call `/engineer` with the following task:**

```
## YOUR TASK: Assess Deployment Readiness

Evaluate whether the sprint changes are ready for deployment to production.

### Sprint Changes
{List completed features and code changes}

### Deployment Readiness Checklist

1. **Build & Package**
   - âœ… Build succeeds without errors
   - âœ… All tests pass
   - âœ… Artifacts generated correctly
   - âœ… Version incremented appropriately

2. **Database Migrations**
   - Migration scripts tested
   - Rollback scripts prepared
   - Data backup plan in place
   - Migration tested in staging

3. **Infrastructure**
   - Required infrastructure provisioned
   - Environment variables configured
   - Secrets management configured
   - Monitoring/alerting configured

4. **Documentation**
   - Release notes prepared
   - Deployment guide updated
   - API documentation updated (if applicable)
   - User documentation updated (if applicable)

5. **Rollback Plan**
   - Rollback procedure documented
   - Rollback tested in staging
   - Rollback decision criteria defined

### Deployment Environment
{% if deployment_config and deployment_config.environments %}
**Environments**: {{ deployment_config.environments | join(', ') }}
{% else %}
**Environments**: dev, staging, prod
{% endif %}

### Output Format

Return deployment readiness assessment:
```json
{
  "ready_for_deployment": true|false,
  "environment": "staging|production",
  "blockers": [
    "Description of any deployment blockers"
  ],
  "deployment_tasks": [
    {
      "task": "Run database migration",
      "owner": "DevOps",
      "estimated_time": "10 minutes"
    }
  ],
  "rollback_ready": true|false,
  "recommendation": "deploy|fix_blockers|deploy_to_staging_first"
}
```
```

**After the agent completes:**
- Review deployment readiness
- Address any blockers

---

## Step 5: Sprint Closure Decision

**Call `/scrum-master` with the following task:**

```
## YOUR TASK: Recommend Sprint Closure Decision

Based on sprint metrics, acceptance tests, security review, and deployment readiness, recommend whether to close the sprint.

### Sprint Metrics
- Completion Rate: {completion_rate}%
- Completed: {completed_points}/{total_points} story points
- Items Not Done: {len(not_done)}

### Acceptance Test Results
{acceptance test summary from Step 2}

### Security Review
{security review summary from Step 3}

### Deployment Readiness
{deployment readiness summary from Step 4}

### Decision Criteria

**Close Sprint (Recommended if):**
- Completion rate >= 80%
- All acceptance tests pass OR failures are minor/acceptable
- No critical/high security vulnerabilities
- Deployment readiness confirmed OR sprint work not deployment-bound

**Extend Sprint (Recommended if):**
- Completion rate < 80% with critical items incomplete
- Acceptance test failures block deployment
- Critical security vulnerabilities need fixing

**Partial Closure (Recommended if):**
- Completion rate 60-80% with some items ready to deploy
- Some features deployable, others need more work

### Output Format

Return sprint closure recommendation:
```markdown
## Sprint Closure Recommendation - {sprint_name}

### Recommendation: CLOSE | EXTEND | PARTIAL_CLOSE

### Rationale
{Explain reasoning based on metrics and reviews}

### Items to Close
{List items that can be closed}

### Items to Carry Over
{List items that should move to next sprint}

### Action Items
1. {Required action 1}
2. {Required action 2}

### Sprint Goal Achievement
- Sprint Goal: {sprint_goal if available}
- Achieved: YES | NO | PARTIAL
```
```

**After the agent completes:**
- Review recommendation
- Prepare for human approval

---

## Step 6: Human Approval Gate

**Present to sprint stakeholders:**

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“Š SPRINT REVIEW SUMMARY - {sprint_name}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“ˆ Completion: {completed_points}/{total_points} pts ({completion_rate:.1f}%)

ðŸ§ª Acceptance Tests: {acceptance_status}
  - Tests Run: {tests_run}
  - Passed: {tests_passed}
  - Failed: {tests_failed}

ðŸ”’ Security: {security_status}
  - Critical Vulns: {critical_count} (Max: {{ quality_standards.critical_vulnerabilities_max }})
  - High Vulns: {high_count} (Max: {{ quality_standards.high_vulnerabilities_max }})

ðŸš€ Deployment: {deployment_ready}

ðŸ“‹ Scrum Master Recommendation: {recommendation}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Choose action:
1. âœ… Close Sprint - Mark sprint complete, deploy changes
2. â¸ï¸  Extend Sprint - Continue work for X more days
3. ðŸ”€ Partial Close - Deploy completed work, carry over rest
4. âŒ Cancel - Review and re-plan

```

**Based on user decision:**

### Option 1: Close Sprint

```python
# Update sprint status
try:
    # Close completed items
    for item in completed:
        adapter.update_work_item(
            work_item_id=item['id'],
            fields={'System.State': 'Done'}
        )

    # Move incomplete items to next sprint
    next_sprint = input("Next sprint name (or 'backlog'): ")
    for item in not_done:
        new_iteration = f'{{ work_tracking.project }}\\{next_sprint}' if next_sprint != 'backlog' else ''
        adapter.update_work_item(
            work_item_id=item['id'],
            fields={'System.IterationPath': new_iteration}
        )

    print(f"âœ… Sprint {sprint_name} closed successfully")
    print(f"   Completed: {len(completed)} items")
    print(f"   Moved to {next_sprint}: {len(not_done)} items")

except Exception as e:
    print(f"âŒ Failed to close sprint: {e}")
```

### Option 2: Extend Sprint

```python
extension_days = input("Extend by how many days: ")
print(f"â¸ï¸  Sprint {sprint_name} extended by {extension_days} days")
print(f"   Focus on completing: {[i['title'] for i in not_done[:5]]}")
```

### Option 3: Partial Close

```python
# User selects which items to deploy
print("Select items to deploy (comma-separated IDs):")
deploy_ids = input("Item IDs: ")
deploy_items = [int(x.strip()) for x in deploy_ids.split(',')]

# Close selected items
# Move others to next sprint
```

---

## Step 7: Generate Sprint Review Report

Save comprehensive report to `.claude/reports/sprint-reviews/`:

```markdown
# Sprint Review Report - {sprint_name}

**Date**: {current_date}
**Decision**: {user_decision}

## Sprint Metrics
- Total Items: {len(sprint_items)} ({total_points} pts)
- Completed: {len(completed)} ({completed_points} pts)
- Completion Rate: {completion_rate:.1f}%

## Acceptance Testing
{acceptance test summary}

## Security Review
{security review summary}

## Deployment Readiness
{deployment readiness summary}

## Sprint Closure
{closure details}

## Lessons Learned
{optional: what went well, what to improve}

## Next Sprint Planning
{items carried over, priorities}
```

---

## Agent Commands Used

| Step | Agent Command | Purpose |
|------|---------------|---------|
| 2 | `/tester` | Run acceptance tests |
| 3 | `/security-specialist` | Final security review |
| 4 | `/engineer` | Deployment readiness |
| 5 | `/scrum-master` | Sprint closure recommendation |

**Key**: Each agent command spawns a **fresh context window** via the Task tool.

---

## Configuration

**Work Tracking Platform:** {{ work_tracking.platform }}

**Quality Standards:**
- Test Coverage: >= {{ quality_standards.test_coverage_min }}%
- Critical Vulnerabilities: <= {{ quality_standards.critical_vulnerabilities_max }}
- High Vulnerabilities: <= {{ quality_standards.high_vulnerabilities_max }}

---

*Generated by Trustable AI Workbench for {{ project.name }}*
*Replaces /sprint-completion (v1.x) with focus on acceptance testing and deployment readiness*
