# Sprint Review Workflow

**Project**: {{ project.name }}
**Workflow**: Sprint Review (Acceptance & Deployment Readiness)
**Purpose**: Review sprint completion, run acceptance tests, assess deployment readiness, and close sprint

## Output Formatting Requirements

**IMPORTANT**: Use actual Unicode emojis in reports, NOT GitHub-style shortcodes.

---

## Workflow Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SPRINT REVIEW - Acceptance & Deployment Readiness                          ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  Step 1: Collect sprint completion metrics                                 ‚îÇ
‚îÇ  Step 1.5: Identify EPICs for testing                                      ‚îÇ
‚îÇ  Step 1.6: Retrieve test plans from work items                             ‚îÇ
‚îÇ  Step 2: /tester ‚Üí Run acceptance tests                                    ‚îÇ
‚îÇ  Step 3: /security-specialist ‚Üí Final security review                      ‚îÇ
‚îÇ  Step 4: /engineer ‚Üí Deployment readiness assessment                       ‚îÇ
‚îÇ  Step 5: /scrum-master ‚Üí Sprint closure decision                           ‚îÇ
‚îÇ  Step 6: Human approval ‚Üí Close sprint or extend                           ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  Each agent command spawns a FRESH CONTEXT WINDOW via Task tool            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Initialize Workflow

```python
# Initialize work tracking adapter
import sys
from datetime import datetime
sys.path.insert(0, ".claude/skills")
from work_tracking import get_adapter

adapter = get_adapter()
print(f"üìã Work Tracking: {adapter.platform}")

sprint_name = input("Sprint name (e.g., Sprint 3): ")

# Load sprint work items
try:
    sprint_items = adapter.query_work_items(
        filters={
            'System.IterationPath': f'{{ work_tracking.project }}\\{sprint_name}'
        }
    )
    print(f"üìã Found {len(sprint_items)} items in {sprint_name}")
except Exception as e:
    print(f"‚ùå Failed to load sprint items: {e}")
    sprint_items = []
```

---

## Step 1: Collect Sprint Completion Metrics

```python
# Calculate completion metrics
completed = [i for i in sprint_items if i.get('state') == 'Done']
in_progress = [i for i in sprint_items if i.get('state') == 'In Progress']
not_done = [i for i in sprint_items if i.get('state') not in ['Done', 'Removed']]

# Get story points
def get_story_points(item):
    {% if work_tracking.custom_fields.story_points %}
    return item.get('fields', {}).get('{{ work_tracking.custom_fields.story_points }}', 0) or 0
    {% else %}
    return 0
    {% endif %}

total_points = sum(get_story_points(i) for i in sprint_items)
completed_points = sum(get_story_points(i) for i in completed)
completion_rate = (completed_points / total_points * 100) if total_points > 0 else 0

print(f"\nüìä Sprint Completion Metrics:")
print(f"  Total Items: {len(sprint_items)} ({total_points} pts)")
print(f"  ‚úÖ Completed: {len(completed)} ({completed_points} pts)")
print(f"  üîÑ In Progress: {len(in_progress)}")
print(f"  ‚¨ú Not Done: {len(not_done)}")
print(f"  üìà Completion Rate: {completion_rate:.1f}%")
```

---

## Step 1.5: Identify EPICs for Testing

**CRITICAL**: Query work tracking adapter for Epic work items scheduled for review in current sprint. Extract EPICs and verify each has attached/linked acceptance test plan. Only EPICs with test plans proceed to acceptance testing (VISION.md Pillar #2: External Source of Truth).

```python
import os
from pathlib import Path

# Identify EPICs in sprint scope
print(f"\nüîç Identifying EPICs for acceptance testing...")
print("=" * 80)
print("CRITICAL: Querying work tracking adapter for Epic work items in sprint scope")
print("=" * 80)

epic_ids = []
epic_data = []

# Query adapter for Epic work items in sprint
try:
    epic_items = adapter.query_work_items(
        filters={
            'System.WorkItemType': '{{ work_tracking.work_item_types.epic }}',
            'System.IterationPath': f'{{ work_tracking.project }}\\{sprint_name}'
        }
    )
    print(f"üìã Found {len(epic_items)} EPIC(s) in {sprint_name}")
except Exception as e:
    print(f"‚ö†Ô∏è  Failed to query Epic work items: {e}")
    epic_items = []

# Extract EPIC IDs and metadata
for item in epic_items:
    try:
        epic_id = item.get('id')
        fields = item.get('fields', {})

        epic_metadata = {
            'id': epic_id,
            'title': item.get('title') or fields.get('System.Title', ''),
            'state': item.get('state') or fields.get('System.State', ''),
            'description': item.get('description') or fields.get('System.Description', '')
        }

        epic_ids.append(epic_id)
        epic_data.append(epic_metadata)
        print(f"‚úÖ Found EPIC #{epic_id}: {epic_metadata['title']}")

    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to extract EPIC metadata: {e}")

# Verify each EPIC has attached test plan
print(f"\nüìã Verifying test plans for {len(epic_data)} EPIC(s)...")
print("=" * 80)

testable_epics = []
epics_without_test_plans = []

for epic in epic_data:
    epic_id = epic['id']
    epic_title = epic['title']
    test_plan_found = False

    print(f"\nüîç Checking EPIC #{epic_id}: {epic_title}")

    try:
        # Platform-specific test plan verification
        if adapter.platform == 'azure-devops':
            # Azure DevOps: Check work item attachments for test plan file
            print(f"   üîó Checking Azure DevOps attachments...")

            # Import Azure CLI wrapper (canonical source)
            import sys
            sys.path.insert(0, ".claude/skills")
            from azure_devops.cli_wrapper import azure_cli

            # Query work item for attachments
            work_item = adapter.get_work_item(epic_id)

            if work_item:
                # Check relations for attachments
                relations = work_item.get('relations', [])

                for relation in relations:
                    rel_type = relation.get('rel', '')

                    # Azure DevOps: AttachedFile relation type
                    if 'AttachedFile' in rel_type:
                        # Get attachment URL
                        attachment_url = relation.get('url', '')

                        # Check if attachment is a test plan (contains 'test-plan' in name)
                        if 'test-plan' in attachment_url.lower():
                            test_plan_found = True
                            print(f"   ‚úÖ Test plan attachment found: {attachment_url}")
                            break

                if not test_plan_found:
                    # Also check for test plan file by expected name pattern
                    expected_filename = f"epic-{epic_id}-test-plan.md"
                    test_plan_found = azure_cli.verify_attachment_exists(
                        work_item_id=int(epic_id) if isinstance(epic_id, int) else int(str(epic_id).split('-')[-1]),
                        filename=expected_filename
                    )

                    if test_plan_found:
                        print(f"   ‚úÖ Test plan file verified: {expected_filename}")

        elif adapter.platform == 'file-based':
            # File-based: Check work item comments for test plan path
            print(f"   üìù Checking file-based work item comments...")

            work_item = adapter.get_work_item(epic_id)

            if work_item:
                comments = work_item.get('comments', [])

                # Check if any comment contains test plan path
                for comment in comments:
                    comment_text = comment.get('text', '')

                    # Look for test plan path in comment
                    if 'Test Plan:' in comment_text or 'test-plan' in comment_text.lower():
                        test_plan_found = True
                        print(f"   ‚úÖ Test plan reference found in comments")
                        break

                if not test_plan_found:
                    # Also check if test plan file exists at expected location
                    test_plan_path = Path(f".claude/acceptance-tests/epic-{epic_id}-test-plan.md")

                    if test_plan_path.exists():
                        test_plan_found = True
                        print(f"   ‚úÖ Test plan file exists: {test_plan_path}")

        else:
            # Unsupported platform - check local file system
            print(f"   üìÅ Checking local file system...")
            test_plan_path = Path(f".claude/acceptance-tests/epic-{epic_id}-test-plan.md")

            if test_plan_path.exists():
                test_plan_found = True
                print(f"   ‚úÖ Test plan file exists: {test_plan_path}")

        # Add EPIC to appropriate list
        if test_plan_found:
            testable_epics.append(epic)
            print(f"   ‚úÖ EPIC #{epic_id} eligible for acceptance testing")
        else:
            epics_without_test_plans.append(epic)
            print(f"   ‚ö†Ô∏è  WARNING: EPIC #{epic_id} has no test plan - excluded from acceptance testing")

    except Exception as e:
        print(f"   ‚ùå ERROR: Failed to verify test plan for EPIC #{epic_id}: {e}")
        epics_without_test_plans.append(epic)

# Output EPIC identification summary
print(f"\nüìä EPIC Identification Summary:")
print(f"   Total EPICs found: {len(epic_data)}")
print(f"   EPICs with test plans: {len(testable_epics)}")
print(f"   EPICs without test plans: {len(epics_without_test_plans)}")

# Log warnings for EPICs without test plans
if epics_without_test_plans:
    print(f"\n‚ö†Ô∏è  WARNING: {len(epics_without_test_plans)} EPIC(s) excluded from acceptance testing due to missing test plans:")
    for epic in epics_without_test_plans:
        print(f"     ‚Ä¢ EPIC #{epic['id']}: {epic['title']}")
    print(f"\n   These EPICs will be skipped during acceptance testing.")
    print(f"   To include them, attach/link test plans to the EPIC work items.")

# Store testable EPICs in workflow state
testable_epics_state = {
    'testable_epics': testable_epics,
    'epics_without_test_plans': epics_without_test_plans,
    'total_epics_found': len(epic_data),
    'testable_count': len(testable_epics),
    'identification_timestamp': datetime.now().isoformat()
}

# Checkpoint: Save testable EPICs to workflow state
# (In production, this would be saved via StateManager)
print(f"\nüíæ Testable EPICs stored in workflow state for acceptance testing")
print(f"=" * 80)
```

---

## Step 1.6: Retrieve Test Plans from Work Items

**CRITICAL**: Retrieve acceptance test plan files from EPIC work item attachments/links. For Azure DevOps: download attached files. For file-based: read from local filesystem. Validate test plan content structure (VISION.md Pillar #2: External Source of Truth).

```python
import json
import re

# Retrieve test plans for testable EPICs
print(f"\nüì• Retrieving test plans for {len(testable_epics)} EPIC(s)...")
print("=" * 80)
print("CRITICAL: Retrieving test plan files from work item attachments/links")
print("=" * 80)

retrieved_test_plans = []
retrieval_failures = []

for epic in testable_epics:
    epic_id = epic['id']
    epic_title = epic['title']
    test_plan_content = None
    test_plan_path = None

    print(f"\nüìÑ Retrieving test plan for EPIC #{epic_id}: {epic_title}")

    try:
        # Platform-specific test plan retrieval
        if adapter.platform == 'azure-devops':
            # Azure DevOps: Download test plan from work item attachment
            print(f"   üîó Downloading from Azure DevOps...")

            # Import Azure CLI wrapper
            import sys
            sys.path.insert(0, ".claude/skills")
            from azure_devops.cli_wrapper import azure_cli

            # Query work item for attachments
            work_item = adapter.get_work_item(epic_id)

            if work_item:
                relations = work_item.get('relations', [])
                attachment_found = False

                # Find test plan attachment
                for relation in relations:
                    rel_type = relation.get('rel', '')

                    if 'AttachedFile' in rel_type:
                        attachment_url = relation.get('url', '')
                        attachment_name = relation.get('attributes', {}).get('name', '')

                        # Check if this is the test plan file
                        if f'epic-{epic_id}-test-plan.md' in attachment_name or 'test-plan' in attachment_name.lower():
                            # Download attachment to .claude/acceptance-tests/
                            test_plans_dir = Path('.claude/acceptance-tests')
                            test_plans_dir.mkdir(parents=True, exist_ok=True)

                            # Download file using Azure CLI
                            download_path = test_plans_dir / f"epic-{epic_id}-test-plan.md"

                            # Use azure_cli to download attachment
                            # Note: Azure CLI doesn't have direct attachment download, so we read from local if exists
                            # In production, would implement azure_cli.download_attachment()
                            if download_path.exists():
                                with open(download_path, 'r', encoding='utf-8') as f:
                                    test_plan_content = f.read()
                                test_plan_path = str(download_path)
                                attachment_found = True
                                print(f"   ‚úÖ Test plan downloaded: {download_path}")
                            else:
                                print(f"   ‚ö†Ô∏è  Test plan file not found locally: {download_path}")
                                print(f"   ‚ÑπÔ∏è  Assuming file was attached but not yet downloaded")
                                # In real implementation, would download from attachment URL
                                # For now, treat as retrieval failure

                            break

                if not attachment_found:
                    # Try reading from local filesystem as fallback
                    test_plan_path_fallback = Path(f".claude/acceptance-tests/epic-{epic_id}-test-plan.md")
                    if test_plan_path_fallback.exists():
                        with open(test_plan_path_fallback, 'r', encoding='utf-8') as f:
                            test_plan_content = f.read()
                        test_plan_path = str(test_plan_path_fallback)
                        print(f"   ‚úÖ Test plan retrieved from filesystem: {test_plan_path_fallback}")

        elif adapter.platform == 'file-based':
            # File-based: Read test plan from local filesystem
            print(f"   üìÅ Reading from local filesystem...")

            # Get test plan path from work item comments or expected location
            work_item = adapter.get_work_item(epic_id)
            test_plan_path_from_comment = None

            if work_item:
                comments = work_item.get('comments', [])

                # Look for test plan path in comments
                for comment in comments:
                    comment_text = comment.get('text', '')

                    # Extract path from comment (format: "Test Plan: path/to/file.md")
                    if 'Test Plan:' in comment_text:
                        # Extract path after "Test Plan:"
                        match = re.search(r'Test Plan:\s*(.+\.md)', comment_text)
                        if match:
                            test_plan_path_from_comment = match.group(1).strip()
                            break

            # Try comment path first, then expected location
            test_plan_paths_to_try = []
            if test_plan_path_from_comment:
                test_plan_paths_to_try.append(Path(test_plan_path_from_comment))
            test_plan_paths_to_try.append(Path(f".claude/acceptance-tests/epic-{epic_id}-test-plan.md"))

            for test_plan_path_candidate in test_plan_paths_to_try:
                if test_plan_path_candidate.exists():
                    with open(test_plan_path_candidate, 'r', encoding='utf-8') as f:
                        test_plan_content = f.read()
                    test_plan_path = str(test_plan_path_candidate)
                    print(f"   ‚úÖ Test plan retrieved: {test_plan_path_candidate}")
                    break

        else:
            # Unsupported platform - try local filesystem
            print(f"   üìÅ Reading from local filesystem (unsupported platform)...")
            test_plan_path_fallback = Path(f".claude/acceptance-tests/epic-{epic_id}-test-plan.md")

            if test_plan_path_fallback.exists():
                with open(test_plan_path_fallback, 'r', encoding='utf-8') as f:
                    test_plan_content = f.read()
                test_plan_path = str(test_plan_path_fallback)
                print(f"   ‚úÖ Test plan retrieved: {test_plan_path_fallback}")

        # Validate test plan content
        if test_plan_content:
            print(f"   üîç Validating test plan content...")

            # Check for expected sections
            required_sections = [
                'EPIC',  # EPIC overview section
                'FEATURE',  # FEATURES covered section
                'Test Case',  # Test cases section
            ]

            missing_sections = []
            for section in required_sections:
                if section.lower() not in test_plan_content.lower():
                    missing_sections.append(section)

            if missing_sections:
                print(f"   ‚ö†Ô∏è  WARNING: Test plan missing expected sections: {', '.join(missing_sections)}")
                print(f"   ‚ÑπÔ∏è  Test plan may be incomplete or malformed")
            else:
                print(f"   ‚úÖ Test plan structure validated")

            # Check file size (basic corruption check)
            if len(test_plan_content) < 100:
                print(f"   ‚ö†Ô∏è  WARNING: Test plan is unusually short ({len(test_plan_content)} bytes)")
                print(f"   ‚ÑπÔ∏è  File may be corrupted or incomplete")
            else:
                print(f"   ‚úÖ Test plan size: {len(test_plan_content)} bytes")

            # Store retrieved test plan
            retrieved_test_plans.append({
                'epic_id': epic_id,
                'epic_title': epic_title,
                'test_plan_path': test_plan_path,
                'test_plan_content': test_plan_content,
                'content_length': len(test_plan_content),
                'validation_warnings': missing_sections if missing_sections else []
            })

            print(f"   ‚úÖ Test plan retrieved successfully for EPIC #{epic_id}")

        else:
            # Test plan not found or couldn't be read
            print(f"   ‚ùå ERROR: Could not retrieve test plan for EPIC #{epic_id}")
            retrieval_failures.append({
                'epic_id': epic_id,
                'epic_title': epic_title,
                'reason': 'Test plan file not found or could not be read'
            })

    except FileNotFoundError as e:
        print(f"   ‚ùå ERROR: Test plan file not found: {e}")
        retrieval_failures.append({
            'epic_id': epic_id,
            'epic_title': epic_title,
            'reason': f'File not found: {e}'
        })

    except PermissionError as e:
        print(f"   ‚ùå ERROR: Permission denied reading test plan: {e}")
        retrieval_failures.append({
            'epic_id': epic_id,
            'epic_title': epic_title,
            'reason': f'Permission denied: {e}'
        })

    except UnicodeDecodeError as e:
        print(f"   ‚ùå ERROR: Test plan file is corrupted or has invalid encoding: {e}")
        retrieval_failures.append({
            'epic_id': epic_id,
            'epic_title': epic_title,
            'reason': f'File corrupted or invalid encoding: {e}'
        })

    except Exception as e:
        print(f"   ‚ùå ERROR: Unexpected error retrieving test plan: {e}")
        retrieval_failures.append({
            'epic_id': epic_id,
            'epic_title': epic_title,
            'reason': f'Unexpected error: {e}'
        })

# Output test plan retrieval summary
print(f"\nüìä Test Plan Retrieval Summary:")
print(f"   Total EPICs with test plans: {len(testable_epics)}")
print(f"   Test plans retrieved successfully: {len(retrieved_test_plans)}")
print(f"   Retrieval failures: {len(retrieval_failures)}")

# Log retrieval failures
if retrieval_failures:
    print(f"\n‚ùå ERROR: Failed to retrieve test plans for {len(retrieval_failures)} EPIC(s):")
    for failure in retrieval_failures:
        print(f"     ‚Ä¢ EPIC #{failure['epic_id']}: {failure['epic_title']}")
        print(f"       Reason: {failure['reason']}")
    print(f"\n   ‚ö†Ô∏è  These EPICs cannot proceed to acceptance testing without test plans")

# Store retrieved test plans in workflow state
retrieved_test_plans_state = {
    'retrieved_test_plans': retrieved_test_plans,
    'retrieval_failures': retrieval_failures,
    'successful_retrievals': len(retrieved_test_plans),
    'failed_retrievals': len(retrieval_failures),
    'retrieval_timestamp': datetime.now().isoformat()
}

# Checkpoint: Save retrieved test plans to workflow state
# (In production, this would be saved via StateManager)
print(f"\nüíæ Retrieved test plans stored in workflow state for test execution")
print(f"=" * 80)

# Update testable EPICs list to only include EPICs with successfully retrieved test plans
testable_epics = [
    epic for epic in testable_epics
    if any(tp['epic_id'] == epic['id'] for tp in retrieved_test_plans)
]

print(f"\nüìã {len(testable_epics)} EPIC(s) ready for acceptance testing with retrieved test plans")
```

---

## Step 2: Run Acceptance Tests

**Call `/tester` with the following task:**

```
## YOUR TASK: Run Acceptance Tests for Sprint Completion

Execute acceptance tests for all completed features in the sprint.

### Sprint Details
- Sprint: {sprint_name}
- Completed Items: {len(completed)}
- Completed Story Points: {completed_points}

### Completed Work Items
{For each completed item:
  - ID: {item['id']}
  - Title: {item['title']}
  - Acceptance Criteria: {item['acceptance_criteria']}
}

### Acceptance Testing Requirements

1. **Functional Acceptance Tests**
   - Verify each completed feature meets its acceptance criteria
   - Test user workflows end-to-end
   - Validate integration with existing system
   - Check for regressions in existing functionality

2. **Non-Functional Acceptance Tests**
   - Performance: Response times within SLA
   - Scalability: Handles expected load
   - Usability: UI/UX meets standards
   - Compatibility: Works across supported platforms/browsers

3. **Data Validation**
   - Data migrations completed successfully
   - Data integrity maintained
   - No data loss or corruption

4. **Quality Gates**
   - Test coverage >= {{ quality_standards.test_coverage_min }}%
   - No critical or high priority bugs open
   - All tests passing
   - No security vulnerabilities

### Output Format

Return JSON with:
```json
{
  "acceptance_status": "pass|fail|partial",
  "tests_run": 50,
  "tests_passed": 48,
  "tests_failed": 2,
  "coverage_percent": 85,
  "failed_criteria": [
    {
      "work_item": "1234",
      "criterion": "User can reset password",
      "failure_reason": "Email not sent in dev environment"
    }
  ],
  "quality_gates": {
    "coverage_met": true,
    "no_critical_bugs": true,
    "all_tests_passing": false
  },
  "recommendation": "approve|fix_required|partial_approval"
}
```
```

**After the agent completes:**
- Parse acceptance test results
- Document any failures

---

## Step 3: Security Review

**Call `/security-specialist` with the following task:**

```
## YOUR TASK: Final Security Review Before Deployment

Perform final security review of all changes in the sprint.

### Sprint Changes
- Completed Features: {list of completed features}
- Code Changes: {git diff statistics}
- New Dependencies: {any new packages/libraries added}

### Security Review Checklist

1. **Vulnerability Scan Results**
   - Run dependency scanner (e.g., pip-audit, npm audit)
   - Check for known vulnerabilities in dependencies
   - Verify {{ quality_standards.critical_vulnerabilities_max }} critical vulns max
   - Verify {{ quality_standards.high_vulnerabilities_max }} high vulns max

2. **Code Security Review**
   - OWASP Top 10 compliance
   - Authentication/authorization changes reviewed
   - Input validation and sanitization
   - SQL injection prevention
   - XSS prevention
   - CSRF protection

3. **Configuration Security**
   - No secrets in code or config files
   - Proper environment variable usage
   - Secure default configurations
   - HTTPS enforced where required

4. **Deployment Security**
   - Container image vulnerabilities scanned
   - Infrastructure as code reviewed
   - Network security rules validated
   - Access controls configured correctly

### Output Format

Return security review report:
```markdown
## Security Review Report - {sprint_name}

### Summary
- ‚úÖ Critical Vulnerabilities: {count} (Max: {{ quality_standards.critical_vulnerabilities_max }})
- ‚úÖ High Vulnerabilities: {count} (Max: {{ quality_standards.high_vulnerabilities_max }})
- Overall Status: APPROVED | CONDITIONAL | REJECTED

### Vulnerabilities Found
{List with severity, component, fix required}

### Security Requirements for Deployment
{List any security controls that must be in place}

### Recommendations
{Security improvements for future sprints}
```
```

**After the agent completes:**
- Review security findings
- Address critical/high vulnerabilities if any

---

## Step 4: Deployment Readiness Assessment

**Call `/engineer` with the following task:**

```
## YOUR TASK: Assess Deployment Readiness

Evaluate whether the sprint changes are ready for deployment to production.

### Sprint Changes
{List completed features and code changes}

### Deployment Readiness Checklist

1. **Build & Package**
   - ‚úÖ Build succeeds without errors
   - ‚úÖ All tests pass
   - ‚úÖ Artifacts generated correctly
   - ‚úÖ Version incremented appropriately

2. **Database Migrations**
   - Migration scripts tested
   - Rollback scripts prepared
   - Data backup plan in place
   - Migration tested in staging

3. **Infrastructure**
   - Required infrastructure provisioned
   - Environment variables configured
   - Secrets management configured
   - Monitoring/alerting configured

4. **Documentation**
   - Release notes prepared
   - Deployment guide updated
   - API documentation updated (if applicable)
   - User documentation updated (if applicable)

5. **Rollback Plan**
   - Rollback procedure documented
   - Rollback tested in staging
   - Rollback decision criteria defined

### Deployment Environment
{% if deployment_config and deployment_config.environments %}
**Environments**: {{ deployment_config.environments | join(', ') }}
{% else %}
**Environments**: dev, staging, prod
{% endif %}

### Output Format

Return deployment readiness assessment:
```json
{
  "ready_for_deployment": true|false,
  "environment": "staging|production",
  "blockers": [
    "Description of any deployment blockers"
  ],
  "deployment_tasks": [
    {
      "task": "Run database migration",
      "owner": "DevOps",
      "estimated_time": "10 minutes"
    }
  ],
  "rollback_ready": true|false,
  "recommendation": "deploy|fix_blockers|deploy_to_staging_first"
}
```
```

**After the agent completes:**
- Review deployment readiness
- Address any blockers

---

## Step 5: Sprint Closure Decision

**Call `/scrum-master` with the following task:**

```
## YOUR TASK: Recommend Sprint Closure Decision

Based on sprint metrics, acceptance tests, security review, and deployment readiness, recommend whether to close the sprint.

### Sprint Metrics
- Completion Rate: {completion_rate}%
- Completed: {completed_points}/{total_points} story points
- Items Not Done: {len(not_done)}

### Acceptance Test Results
{acceptance test summary from Step 2}

### Security Review
{security review summary from Step 3}

### Deployment Readiness
{deployment readiness summary from Step 4}

### Decision Criteria

**Close Sprint (Recommended if):**
- Completion rate >= 80%
- All acceptance tests pass OR failures are minor/acceptable
- No critical/high security vulnerabilities
- Deployment readiness confirmed OR sprint work not deployment-bound

**Extend Sprint (Recommended if):**
- Completion rate < 80% with critical items incomplete
- Acceptance test failures block deployment
- Critical security vulnerabilities need fixing

**Partial Closure (Recommended if):**
- Completion rate 60-80% with some items ready to deploy
- Some features deployable, others need more work

### Output Format

Return sprint closure recommendation:
```markdown
## Sprint Closure Recommendation - {sprint_name}

### Recommendation: CLOSE | EXTEND | PARTIAL_CLOSE

### Rationale
{Explain reasoning based on metrics and reviews}

### Items to Close
{List items that can be closed}

### Items to Carry Over
{List items that should move to next sprint}

### Action Items
1. {Required action 1}
2. {Required action 2}

### Sprint Goal Achievement
- Sprint Goal: {sprint_goal if available}
- Achieved: YES | NO | PARTIAL
```
```

**After the agent completes:**
- Review recommendation
- Prepare for human approval

---

## Step 6: Human Approval Gate

**Present to sprint stakeholders:**

```
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìä SPRINT REVIEW SUMMARY - {sprint_name}
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üìà Completion: {completed_points}/{total_points} pts ({completion_rate:.1f}%)

üß™ Acceptance Tests: {acceptance_status}
  - Tests Run: {tests_run}
  - Passed: {tests_passed}
  - Failed: {tests_failed}

üîí Security: {security_status}
  - Critical Vulns: {critical_count} (Max: {{ quality_standards.critical_vulnerabilities_max }})
  - High Vulns: {high_count} (Max: {{ quality_standards.high_vulnerabilities_max }})

üöÄ Deployment: {deployment_ready}

üìã Scrum Master Recommendation: {recommendation}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Choose action:
1. ‚úÖ Close Sprint - Mark sprint complete, deploy changes
2. ‚è∏Ô∏è  Extend Sprint - Continue work for X more days
3. üîÄ Partial Close - Deploy completed work, carry over rest
4. ‚ùå Cancel - Review and re-plan

```

**Based on user decision:**

### Option 1: Close Sprint

```python
# Update sprint status
try:
    # Close completed items
    for item in completed:
        adapter.update_work_item(
            work_item_id=item['id'],
            fields={'System.State': 'Done'}
        )

    # Move incomplete items to next sprint
    next_sprint = input("Next sprint name (or 'backlog'): ")
    for item in not_done:
        new_iteration = f'{{ work_tracking.project }}\\{next_sprint}' if next_sprint != 'backlog' else ''
        adapter.update_work_item(
            work_item_id=item['id'],
            fields={'System.IterationPath': new_iteration}
        )

    print(f"‚úÖ Sprint {sprint_name} closed successfully")
    print(f"   Completed: {len(completed)} items")
    print(f"   Moved to {next_sprint}: {len(not_done)} items")

except Exception as e:
    print(f"‚ùå Failed to close sprint: {e}")
```

### Option 2: Extend Sprint

```python
extension_days = input("Extend by how many days: ")
print(f"‚è∏Ô∏è  Sprint {sprint_name} extended by {extension_days} days")
print(f"   Focus on completing: {[i['title'] for i in not_done[:5]]}")
```

### Option 3: Partial Close

```python
# User selects which items to deploy
print("Select items to deploy (comma-separated IDs):")
deploy_ids = input("Item IDs: ")
deploy_items = [int(x.strip()) for x in deploy_ids.split(',')]

# Close selected items
# Move others to next sprint
```

---

## Step 7: Generate Sprint Review Report

Save comprehensive report to `.claude/reports/sprint-reviews/`:

```markdown
# Sprint Review Report - {sprint_name}

**Date**: {current_date}
**Decision**: {user_decision}

## Sprint Metrics
- Total Items: {len(sprint_items)} ({total_points} pts)
- Completed: {len(completed)} ({completed_points} pts)
- Completion Rate: {completion_rate:.1f}%

## Acceptance Testing
{acceptance test summary}

## Security Review
{security review summary}

## Deployment Readiness
{deployment readiness summary}

## Sprint Closure
{closure details}

## Lessons Learned
{optional: what went well, what to improve}

## Next Sprint Planning
{items carried over, priorities}
```

---

## Agent Commands Used

| Step | Agent Command | Purpose |
|------|---------------|---------|
| 2 | `/tester` | Run acceptance tests |
| 3 | `/security-specialist` | Final security review |
| 4 | `/engineer` | Deployment readiness |
| 5 | `/scrum-master` | Sprint closure recommendation |

**Key**: Each agent command spawns a **fresh context window** via the Task tool.

---

## Configuration

**Work Tracking Platform:** {{ work_tracking.platform }}

**Quality Standards:**
- Test Coverage: >= {{ quality_standards.test_coverage_min }}%
- Critical Vulnerabilities: <= {{ quality_standards.critical_vulnerabilities_max }}
- High Vulnerabilities: <= {{ quality_standards.high_vulnerabilities_max }}

---

*Generated by Trustable AI Workbench for {{ project.name }}*
*Replaces /sprint-completion (v1.x) with focus on acceptance testing and deployment readiness*
